{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall nltk -y\n",
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du4TkmHLS2VX",
        "outputId": "81d4c3f1-a127-48b3-b154-8e5f6e9a3ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import simple\n",
        "\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "extracted_files = [\n",
        "    \"/content/Leroy sane_extracted.txt\",\n",
        "    \"/content/Marco asensio_extracted.txt\",\n",
        "    \"/content/Oussmane dembele_extracted.txt\",\n",
        "]\n",
        "\n",
        "# Define a text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Define a simple tokenization function using split\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to compute sentiment scores\n",
        "def sentiment_scores(text):\n",
        "    return sia.polarity_scores(text)\n",
        "\n",
        "# Process each extracted file individually\n",
        "for file_name in extracted_files:\n",
        "    if os.path.exists(file_name):\n",
        "        print(f\"\\nProcessing file: {file_name}\")\n",
        "\n",
        "        # Read the file content\n",
        "        with open(file_name, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Clean and tokenize the text\n",
        "        cleaned = clean_text(content)\n",
        "        tokens = tokenize_text(cleaned)\n",
        "        sentiment = sentiment_scores(cleaned)\n",
        "\n",
        "        # Prepare a DataFrame to store the processed data\n",
        "        df_processed = pd.DataFrame({\n",
        "            \"FileName\": [file_name],\n",
        "            \"OriginalText\": [content],\n",
        "            \"CleanedText\": [cleaned],\n",
        "            \"Tokens\": [tokens],\n",
        "            \"Sentiment\": [sentiment]\n",
        "        })\n",
        "\n",
        "        # Define the output CSV file name (saves in the current directory)\n",
        "        output_file = f\"/content/processed_{os.path.basename(file_name).replace('.txt', '')}.csv\"\n",
        "\n",
        "        # Save the DataFrame to CSV\n",
        "        df_processed.to_csv(output_file, index=False)\n",
        "        print(f\"Processed data saved to '{output_file}'\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxPyaPLpS56X",
        "outputId": "6a25e33b-2e32-4682-fb73-f92c88005fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing file: /content/Leroy sane_extracted.txt\n",
            "Processed data saved to '/content/processed_Leroy sane_extracted.csv'\n",
            "\n",
            "Processing file: /content/Marco asensio_extracted.txt\n",
            "Processed data saved to '/content/processed_Marco asensio_extracted.csv'\n",
            "\n",
            "Processing file: /content/Oussmane dembele_extracted.txt\n",
            "Processed data saved to '/content/processed_Oussmane dembele_extracted.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a text cleaning and preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Normalize text: lowercase, remove punctuation, remove numbers, handle contractions\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)  # Example contraction\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Lemmatization, removing stopwords, and extracting original tokens\n",
        "    doc = nlp(text)\n",
        "    lemmas = []\n",
        "    original_tokens = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and token.lemma_ != '-PRON-':\n",
        "            lemmas.append(token.lemma_)\n",
        "        if not token.is_punct:\n",
        "            original_tokens.append(token.text)  # Collecting original tokens\n",
        "\n",
        "    return \" \".join(lemmas), original_tokens\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to compute sentiment scores\n",
        "def sentiment_scores(text):\n",
        "    return sia.polarity_scores(text)\n",
        "\n",
        "# List of extracted text file names\n",
        "extracted_files = [\n",
        "    \"/content/Leroy sane_extracted.txt\",\n",
        "    \"/content/Marco asensio_extracted.txt\",\n",
        "    \"/content/Oussmane dembele_extracted.txt\",\n",
        "    \"/content/Untitled Folder/extracted_603010077-Neymar-Jr (1).txt\",\n",
        "    \"/content/Untitled Folder/extracted_Young_Kings_Marcus_Rashford_and_Theopolitical_Char (1).txt\",\n",
        "    \"/content/Untitled Folder/extracted_pdf.raphinha (1).txt\"\n",
        "]\n",
        "\n",
        "# Process each extracted file individually\n",
        "for file_name in extracted_files:\n",
        "    if os.path.exists(file_name):\n",
        "        print(f\"\\nProcessing file: {file_name}\")\n",
        "\n",
        "        # Read the file content\n",
        "        with open(file_name, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Clean, lemmatize, remove stopwords, and tokenize\n",
        "        preprocessed_text, tokens = preprocess_text(content)\n",
        "        sentiment = sentiment_scores(preprocessed_text)\n",
        "\n",
        "        # Prepare a DataFrame to store the processed data\n",
        "        df_processed = pd.DataFrame({\n",
        "            \"FileName\": [file_name],\n",
        "            \"OriginalText\": [content],\n",
        "            \"PreprocessedText\": [preprocessed_text],\n",
        "            \"Tokens\": [tokens],\n",
        "            \"Sentiment\": [sentiment]\n",
        "        })\n",
        "\n",
        "        # Define the output CSV file name\n",
        "        output_file = f\"/content/processed_Lemmatization_{os.path.basename(file_name).replace('.txt', '')}.csv\"\n",
        "\n",
        "        # Save the DataFrame to CSV\n",
        "        df_processed.to_csv(output_file, index=False)\n",
        "        print(f\"Processed data saved to '{output_file}'\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_name}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exkdz_O0qoyu",
        "outputId": "3a865d44-2f25-45d7-b196-e21b744d8ef9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing file: /content/Leroy sane_extracted.txt\n",
            "Processed data saved to '/content/processed_Lemmatization_Leroy sane_extracted.csv'\n",
            "\n",
            "Processing file: /content/Marco asensio_extracted.txt\n",
            "Processed data saved to '/content/processed_Lemmatization_Marco asensio_extracted.csv'\n",
            "\n",
            "Processing file: /content/Oussmane dembele_extracted.txt\n",
            "Processed data saved to '/content/processed_Lemmatization_Oussmane dembele_extracted.csv'\n",
            "\n",
            "Processing file: /content/Untitled Folder/extracted_603010077-Neymar-Jr (1).txt\n",
            "Processed data saved to '/content/processed_Lemmatization_extracted_603010077-Neymar-Jr (1).csv'\n",
            "\n",
            "Processing file: /content/Untitled Folder/extracted_Young_Kings_Marcus_Rashford_and_Theopolitical_Char (1).txt\n",
            "Processed data saved to '/content/processed_Lemmatization_extracted_Young_Kings_Marcus_Rashford_and_Theopolitical_Char (1).csv'\n",
            "\n",
            "Processing file: /content/Untitled Folder/extracted_pdf.raphinha (1).txt\n",
            "Processed data saved to '/content/processed_Lemmatization_extracted_pdf.raphinha (1).csv'\n"
          ]
        }
      ]
    }
  ]
}